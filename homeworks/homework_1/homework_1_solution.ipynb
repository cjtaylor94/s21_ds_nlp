{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be analyzing a number of articles. These articles are derived from a number of different news sections. For example, some will be from the politics section and some will be from entertainment, etc. Some documents will be somewhat duplicative.\n",
    "\n",
    "What we would like to do is get an idea of the type of vocabulary employed across all the documents. The hope is that by knowing the vocabulary across these documents we might identify certain words more associated with particular topics. Thus, we might expect that enterntainment might have the word \"film\" whereas politics might have the word \"vote.\"\n",
    "\n",
    "So we will want to generate several pieces of data. Your code should produce this information:\n",
    "- we want a list of all the unique words in this document corpus (You will want to consider lowercasing, stemming, and potentially lemmatization)\n",
    "- we want a list of the words in this document ranked by their frequency\n",
    "- we want a list of the words average frequency in documents (this indicates on average how many times does this term occur in a document\n",
    "- we want to identify what words documents contain unique words in the corpus (imagine a document mentioning a person with a distinct last name. It's possible that that name might only occur once in one document. We want to find all those documents.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_base = \"/Users/teacher/repos/s21_ds_nlp/homeworks/homework_1/data\" \n",
    "# point this to the data directory\n",
    "\n",
    "# you can use the below code to read all of the text files and then have them available in a list\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename, encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(os.path.join(directory, f))\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# here we will generate the list that contains all the files and their contents\n",
    "text_corpus = read_directory_files(dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the words in a document\n",
    "def document_words(document):\n",
    "    word_list = []\n",
    "    analyzed = nlp(document)\n",
    "    for token in analyzed:\n",
    "        #print(token)\n",
    "        # you will want to look at spacy's documentation on token properties\n",
    "        # https://spacy.io/api/token/\n",
    "        # This has the properties you can use in handling the individual tokens\n",
    "        # check to see if the token is alphabetical. \n",
    "        # Also, make sure to check that it isn't a stop word.\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            possible_add = token.lemma_.lower()\n",
    "            if possible_add not in word_list:\n",
    "                word_list.append(possible_add)\n",
    "        # here add to the word_list list\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the words in a document\n",
    "def word_frequency(document):\n",
    "    document_word_frequency_list = []\n",
    "    doc_word_frequency_dict = {}\n",
    "    analyzed = nlp(document)\n",
    "    for token in analyzed:\n",
    "        # you will want to look at spacy's documentation on token properties\n",
    "        # https://spacy.io/api/token/\n",
    "        # This has the properties you can use in handling the individual tokens\n",
    "        \n",
    "        # here add to the word_list list\n",
    "        # you will want to have something to capture the frequency of the word inside the document here.\n",
    "        # you want the document word frequency list as you will want to be able to get document level count info\n",
    "        \n",
    "        \n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            possible_add = token.lemma_.lower()\n",
    "            document_word_frequency_list.append(possible_add)\n",
    "\n",
    "    # now iterate over the set of all words and count them\n",
    "    for token in document_word_frequency_list:\n",
    "        if token not in doc_word_frequency_dict.keys():\n",
    "            doc_word_frequency_dict[token] = 0\n",
    "        doc_word_frequency_dict[token] = doc_word_frequency_dict[token] + 1\n",
    "    \n",
    "    token_count_tuples = [(doc_word_frequency_dict[token], token) for token in doc_word_frequency_dict]\n",
    "    sorted_frequency = sorted(token_count_tuples, reverse=True)\n",
    "    # now that we've got the word list overall\n",
    "    \n",
    "    return sorted_frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration example. This will iterate over every document.\n",
    "\n",
    "# This will be the place where we store \n",
    "all_unique_words = []\n",
    "\n",
    "all_doc_terms = []\n",
    "\n",
    "all_doc_frequency = {}\n",
    "\n",
    "for doc in text_corpus:\n",
    "    #print(doc[\"content\"]) # the corpus is stored in a hash here. You can get the text by looking at the content key\n",
    "    doc_unique_words = document_words(doc[\"content\"])\n",
    "    \n",
    "    # here we can add the unique word list so that we can reuse it later without having to reprocess all docs\n",
    "    all_doc_terms.append(doc_unique_words)\n",
    "    \n",
    "    # Iterate over doc unique words, see if we should add them to master list\n",
    "    for term in doc_unique_words:\n",
    "        if term not in all_unique_words:\n",
    "            all_unique_words.append(term)\n",
    "    \n",
    "    doc_frequency = word_frequency(doc[\"content\"])\n",
    "    for freq, token in doc_frequency:\n",
    "        if token not in all_doc_frequency.keys():\n",
    "            all_doc_frequency[token] = []\n",
    "        all_doc_frequency[token].append(freq)\n",
    "\n",
    "# now we have a dictionary of the count that each term has across all documents\n",
    "term_average_frequency = []\n",
    "for term in all_doc_frequency.keys():\n",
    "    num_docs = len(all_doc_frequency[term])\n",
    "    average_times = sum(all_doc_frequency[term])/num_docs\n",
    "    term_info = (term, num_docs, average_times)\n",
    "    # here we just have a tuple of the term, the number of docs, and the average times\n",
    "    term_average_frequency.append(term_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Iteration Example.\n",
    "# We don't want to iterate over every combination twice\n",
    "\n",
    "all_doc_unique_words = []\n",
    "\n",
    "for index_i, doc_i in enumerate(text_corpus):\n",
    "    temp_row = [] # this will be how we accumulate the upper portion of the matrix\n",
    "    doc_i_unique_words = all_doc_terms[index_i]\n",
    "    doc_i_set = set(doc_i_unique_words)\n",
    "    \n",
    "    for index_j, doc_j in enumerate(text_corpus):\n",
    "        if index_j == index_i:\n",
    "            continue\n",
    "        # here we will want to get the word list from doc_i and compare it to the word list from doc_j\n",
    "        # This should tell us which words are unique between the two\n",
    "        # We can then accumulate the unique words per document and at the end of iterating through all\n",
    "        # documents we should be able to get which words are actually unique in the document.\n",
    "        doc_j_unique_words = all_doc_terms[index_j]\n",
    "        doc_j_set = set(doc_j_unique_words)\n",
    "        # here we see what words are unique for doc i\n",
    "        # we can then take the doc_i_set and assign it the uniqe words between these two\n",
    "        # gradually we will find which terms are unique to the input document\n",
    "        doc_i_set = doc_i_set - doc_j_set\n",
    "        temp_row.append(i_words)\n",
    "    all_doc_unique_words.append(doc_i_set)\n",
    "\n",
    "#print(all_doc_unique_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a brief report of your approach, your results, and any conclusions you can draw from this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credit: \n",
    "\n",
    "Try to identify potential irregular verbs automatically (abuse the fact that the lemmas and the surface forms of irregular verbs are typically different. For example \"eat\" and \"ate\" have similar lemmas but different surface forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
